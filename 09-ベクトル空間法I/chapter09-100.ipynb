{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80. コーパスの整形\n",
    "\n",
    "文を単語列に変換する最も単純な方法は，空白文字で単語に区切ることである．ただ，この方法では文末のピリオドや括弧などの記号が単語に含まれてしまう． そこで，コーパスの各行のテキストを空白文字でトークンのリストに分割した後，各トークンに以下の処理を施し，単語から記号を除去せよ．\n",
    "- トークンの先頭と末尾に出現する次の文字を削除: `.,!?;:()[]'\"`\n",
    "- 空文字列となったトークンは削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = []\n",
    "def get_corpus():\n",
    "    if Path('./data/corpus100_80.txt').exists():\n",
    "        return\n",
    "    with open('./data/enwiki-20150112-400-r100-10576.txt', 'r') as f_in, \\\n",
    "         open('./data/corpus100_80.txt', 'w') as f_out:\n",
    "        corpus = []\n",
    "        for line in f_in.readlines():\n",
    "            line = [word.strip('.,!?:;()[]\\'\\\"') for word in line.split() \\\n",
    "                       if word.strip('.,!?:;()[]\\'\\\"')!='']\n",
    "            if len(line) >= 1:\n",
    "                corpus + line\n",
    "                f_out.write(' '.join(line)+'\\n')\n",
    "get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Anarchism\n",
      "1 Anarchism is a political philosophy that advocates stateless societies often defined as self-governed voluntary institutions but that several authors have defined as more specific institutions based on non-hierarchical free associations Anarchism holds the state to be undesirable unnecessary or harmful While anti-statism is central anarchism entails opposing authority or hierarchical organisation in the conduct of human relations including but not limited to the state system\n",
      "2 As a subtle and anti-dogmatic philosophy anarchism draws on many currents of thought and strategy Anarchism does not offer a fixed body of doctrine from a single particular world view instead fluxing and flowing as a philosophy There are many types and traditions of anarchism not all of which are mutually exclusive Anarchist schools of thought can differ fundamentally supporting anything from extreme individualism to complete collectivism Strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications Anarchism is usually considered a radical left-wing ideology and much of anarchist economics and anarchist legal philosophy reflect anti-authoritarian interpretations of communism collectivism syndicalism mutualism or participatory economics\n",
      "3 The central tendency of anarchism as a social movement has been represented by anarcho-communism and anarcho-syndicalism with individualist anarchism being primarily a literary phenomenon which nevertheless did have an impact on the bigger currents and individualists have also participated in large anarchist organisations Many anarchists oppose all forms of aggression supporting self-defense or non-violence anarcho-pacifism while others have supported the use of some coercive measures including violent revolution and propaganda of the deed as means to achieve anarchist ends\n",
      "4 Etymology and terminology\n"
     ]
    }
   ],
   "source": [
    "for line in zip(range(5), open('./data/corpus100_80.txt')):\n",
    "    print(*line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 81. 複合語からなる国名への対処\n",
    "英語では，複数の語の連接が意味を成すことがある．例えば，アメリカ合衆国は\"United States\"，イギリスは\"United Kingdom\"と表現されるが，\"United\"や\"States\"，\"Kingdom\"という単語だけでは，指し示している概念・実体が曖昧である．そこで，コーパス中に含まれる複合語を認識し，複合語を1語として扱うことで，複合語の意味を推定したい．しかしながら，複合語を正確に認定するのは大変むずかしいので，ここでは複合語からなる国名を認定したい\n",
    "\n",
    "~~国名データは[ここ](https://www.worldometers.info/geography/alphabetical-list-of-countries/)から手に入れる~~\n",
    "マン島がなくてダメだった．\n",
    "\n",
    "腹が立ったけど[ISO 3166-1に存在する国](http://www.asahi-net.or.jp/~ax2s-kmtn/ref/iso3166-1.html)から取得することにした．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def get_countriy_name():\n",
    "    url = 'https://www.listofcountriesoftheworld.com/'\n",
    "    save_path = './data/countries.txt'\n",
    "    if os.path.exists(save_path):\n",
    "        return\n",
    "    # Setup Driver\n",
    "    print('Setup WebDriver')\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--blink-settings=imageEnabled=false')\n",
    "    browser = webdriver.Chrome(executable_path='./chromedriver', options=options)\n",
    "    browser.set_page_load_timeout(10*62)\n",
    "    \n",
    "    # Scraping\n",
    "    try:\n",
    "        browser.get(url)\n",
    "        table_contents = browser.find_element_by_xpath('//*[@id=\"left-col\"]')\n",
    "        rows = table_contents.find_elements_by_xpath('//*[@id=\"ctry\"]') # //*[@id=\"ctry\"]\n",
    "        with open(save_path, 'w') as f:\n",
    "            for row in rows:\n",
    "                content = row.find_element_by_tag_name('a')\n",
    "                print(content)\n",
    "                f.write(content.text+'\\n')\n",
    "#        countries = '\\n'.join([row.find_elements_by_tag_name('td')[1].text for row in rows])\n",
    "        print('Scraping is success.')\n",
    "    except Exception as ERROR:\n",
    "        print(f'[TimeoutException]: {ERROR}')\n",
    "    browser.close()\n",
    "\n",
    "get_countriy_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "# from tqdm import tqdm_notebook, tqdm\n",
    "countries = list(pycountry.countries)\n",
    "countries_compound = [country.name for country in countries if ' ' in country.name]\n",
    "with open('./data/countries.txt', 'w') as f:\n",
    "    f.write('\\n'.join(['_'.join(name.split()) for name in countries_compound]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/corpus100_81.txt', 'w')as f_81:\n",
    "    for line in open('./data/corpus100_80.txt'):\n",
    "        for name in countries_compound:\n",
    "            if name in line:\n",
    "                line = line.replace(name, '_'.join(name.split()))\n",
    "        f_81.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sainsbury African Galleries display 600 objects from the greatest permanent collection of African arts and culture in the world The three permanent galleries provide a substantial exhibition space for the Museum's African collection comprising over 200,000 objects A curatorial scope that encompasses both archaeological and contemporary material including both unique masterpieces of artistry and objects of everyday life Highlights of the African collection include the Benin Bronzes the beautiful Bronze Head of Queen Idia a magnificent brass head of a Yoruba ruler from Ife the Apapa Hoard from Lagos southern Nigeria Asante goldwork from Ghana including the Bowdich collection the rare Akan Drum from the same region in West Africa a series of soapstone figures from the Kissi people in Sierra_Leone and Liberia the Torday collection of Central African sculpture textiles and weaponry important material from Ethiopia following the British Expedition to Abyssinia the unique Luzira Head from Uganda excavated objects from Great Zimbabwe and satellite towns such as Mutare including a large hoard of Iron Age soapstone figures and cave paintings and petroglyphs from South_Africa\n",
      "\n",
      "Napoleon's son Napoléon François Charles Joseph 1811–1832 was created King of Rome 1811–1814 and was later styled Napoléon II by loyalists of the dynasty though he only ruled for two weeks after his father's abdication Louis-Napoléon 1808–1873 son of Louis was President of France in 1848–1852 and Emperor in 1852–1870 reigning as Napoleon III his son Napoléon Prince Imperial 1856–1879 died fighting the Zulus in Natal South_Africa With his death the family lost much of its remaining political appeal though claimants continue to assert their right to the imperial title A political movement for Corsican independence surfaced in the 1990s which included a Bonapartist restoration in its programme\n",
      "\n",
      "The Columbia area was once part of the Mississippian culture and home to the Mound Builders When European explorers arrived the area was populated by the Osage and Missouri Indians In 1678 La Salle claimed all of Missouri for France The Lewis and Clark Expedition passed by the area on the Missouri River in early June 1804 In 1806 two sons of Daniel Boone established a salt lick northwest of Columbia giving the area its early name Boonslick The Boone's Lick Trail wound from St Charles to the lick in present day Howard County In 1818 a group of settlers incorporated under the Smithton Land Company purchased over and established the village of Smithton less than a mile from current day downtown Columbia In 1821 the settlers moved because of lack of water across the Flat Branch to the plateau between the Flat Branch and Hinkson creeks in what is now the downtown district They renamed the settlement Columbia—a poetic personification of the United_States\n",
      "\n",
      "The population generally supports progressive causes such as the extensive city recycling programs and the decriminalization of cannabis both for medical and recreational use at the municipal level though the scope of latter of the two cannabis ordinances has since been restricted The city is also one of only four in the state to offer medical benefits to same-sex partners of city employees The new health plan also extends health benefits to unmarried heterosexual domestic partners of city employees On October 10 2006 the City Council approved an ordinance to prohibit smoking in public places including restaurants and bars The ordinance was passed with protest and several amendments to the ordinance reflect this Today's Columbians are unusually highly educated over half of citizens possess at least a bachelor's degree while over a quarter hold a graduate degree making it the thirteenth most highly educated municipality in the United_States\n",
      "\n",
      "New_Zealand\n",
      "\n",
      "United_States\n",
      "\n",
      "Politics of the Faroe_Islands\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for line in open('./data/corpus100_81.txt'):\n",
    "    for ctry in open('./data/countries.txt'):\n",
    "        if ctry in line:\n",
    "            print(line)\n",
    "            n += 1\n",
    "            continue\n",
    "    if n == 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 82. 文脈の抽出\n",
    "81で作成したコーパス中に出現するすべての単語tに関して，単語tと文脈語cのペアをタブ区切り形式ですべて書き出せ．ただし，文脈語の定義は次の通りとする．\n",
    "\n",
    "ある単語tの前後d単語を文脈語cとして抽出する（ただし，文脈語に単語tそのものは含まない）\n",
    "単語tを選ぶ度に，文脈幅dは{1,2,3,4,5}の範囲でランダムに決める．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f68f2fe57de46ee8d6b4af5e36e312d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "\n",
    "with open('./data/answer100_82.txt', 'w') as f_82:\n",
    "    for line in tqdm_notebook(open('./data/corpus100_81.txt')):\n",
    "        words = line.split()\n",
    "        for index, t in enumerate(words):\n",
    "            d = random.randint(1, 5)\n",
    "            start = max(index - d, 0)\n",
    "            end = index + d + 1\n",
    "            for c in words[start:index] + words[index+1:end]:\n",
    "                f_82.write(f'{t}\\t{c}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\tis\n",
      "\n",
      "Anarchism\ta\n",
      "\n",
      "is\tAnarchism\n",
      "\n",
      "is\ta\n",
      "\n",
      "is\tpolitical\n",
      "\n",
      "is\tphilosophy\n",
      "\n",
      "is\tthat\n",
      "\n",
      "is\tadvocates\n",
      "\n",
      "a\tAnarchism\n",
      "\n",
      "a\tis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, context in enumerate(open('./data/answer100_82.txt')):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 83. 単語／文脈の頻度の計測\n",
    "82の出力を利用し，以下の出現分布，および定数を求めよ．\n",
    "\n",
    "- f(t,c): 単語tと文脈語cの共起回数\n",
    "- f(t,∗): 単語tの出現回数\n",
    "- f(∗,c): 文脈語cの出現回数\n",
    "- N: 単語と文脈語のペアの総出現回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c3a10e7b5c4904b8884b703ec51499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N = 68091097\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# f(t,c)\n",
    "f_tc_counter = Counter()\n",
    "f_t_counter = Counter()\n",
    "f_c_counter = Counter()\n",
    "\n",
    "tc_list = []\n",
    "t_list = []\n",
    "c_list = []\n",
    "num = 1000000\n",
    "for i ,line in enumerate(tqdm_notebook(open('./data/answer100_82.txt'))):\n",
    "    tc = line.strip()\n",
    "    t, c = tc.split('\\t')\n",
    "    tc_list.append(tc)\n",
    "    t_list.append(t)\n",
    "    c_list.append(c)\n",
    "    \n",
    "    if i % num == 0:\n",
    "        f_tc_counter.update(tc_list)\n",
    "        tc_list = []\n",
    "        f_t_counter.update(t_list)\n",
    "        t_list = []\n",
    "        f_c_counter.update(c_list)\n",
    "        c_list = []\n",
    "f_tc_counter.update(tc_list)\n",
    "f_t_counter.update(t_list)\n",
    "f_c_counter.update(c_list)\n",
    "\n",
    "with open('./data/f_tc_counter.pkl', 'wb') as counter_file:\n",
    "    pickle.dump(f_tc_counter, counter_file)\n",
    "with open('./data/f_t_counter.pkl', 'wb') as counter_file:\n",
    "    pickle.dump(f_t_counter, counter_file)\n",
    "with open('./data/f_c_counter.pkl', 'wb') as counter_file:\n",
    "    pickle.dump(f_c_counter, counter_file)\n",
    "print(f'N = {sum([i for i in f_t_counter.values()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f_tc_counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-996166576938>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/f_tc_counter.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcounter_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mpickle_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_tc_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/f_t_counter.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcounter_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mpickle_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_t_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_tc_counter' is not defined"
     ]
    }
   ],
   "source": [
    "class MacOSFile(object):\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self.f, item)\n",
    "\n",
    "    def read(self, n):\n",
    "        # print(\"reading total_bytes=%s\" % n, flush=True)\n",
    "        if n >= (1 << 31):\n",
    "            buffer = bytearray(n)\n",
    "            idx = 0\n",
    "            while idx < n:\n",
    "                batch_size = min(n - idx, 1 << 31 - 1)\n",
    "                # print(\"reading bytes [%s,%s)...\" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "                buffer[idx:idx + batch_size] = self.f.read(batch_size)\n",
    "                # print(\"done.\", flush=True)\n",
    "                idx += batch_size\n",
    "            return buffer\n",
    "        return self.f.read(n)\n",
    "\n",
    "    def write(self, buffer):\n",
    "        n = len(buffer)\n",
    "        print(\"writing total_bytes=%s...\" % n, flush=True)\n",
    "        idx = 0\n",
    "        while idx < n:\n",
    "            batch_size = min(n - idx, 1 << 31 - 1)\n",
    "            print(\"writing bytes [%s, %s)... \" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "            self.f.write(buffer[idx:idx + batch_size])\n",
    "            print(\"done.\", flush=True)\n",
    "            idx += batch_size\n",
    "\n",
    "def pickle_dump(obj, file_path):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        return pickle.dump(obj, MacOSFile(f), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "def pickle_load(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(MacOSFile(f))\n",
    "\n",
    "with open('./data/f_tc_counter.pkl', 'wb') as counter_file:\n",
    "    pickle_dump(f_tc_counter, counter_file)\n",
    "with open('./data/f_t_counter.pkl', 'wb') as counter_file:\n",
    "    pickle_dump(f_t_counter, counter_file)\n",
    "with open('./data/f_c_counter.pkl', 'wb') as counter_file:\n",
    "    pickle_dump(f_c_counter, counter_file)\n",
    "print(f'N = {sum([i for i in f_t_counter.values()])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 84. 単語文脈行列の作成\n",
    "83の出力を利用し，単語文脈行列XXを作成せよ．ただし，行列XXの各要素XtcXtcは次のように定義する．\n",
    "\n",
    "- $f(t,c) \\geq 10$ ならば， $X_{tc} = PPMI(t,c) = \\max\\{\\log {\\frac{N \\times f(t,c)}{f(t,∗)\\times f(∗,c)}},0\\}$\n",
    "- $f(t,c)<10f(t,c)<10ならば，Xtc=0Xtc=0 $\n",
    "ここで，PPMI(tt,cc)はPositive Pointwise Mutual Information（正の相互情報量）と呼ばれる統計量である．なお，行列XXの行数・列数は数百万オーダとなり，行列のすべての要素を主記憶上に載せることは無理なので注意すること．幸い，行列XXのほとんどの要素は0になるので，非0の要素だけを書き出せばよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from scipy import sparse, io\n",
    "import pickle\n",
    "import numpy as np\n",
    "with open('./data/f_tc_counter.pkl', 'rb') as counter_file:\n",
    "    f_tc_counter = pickle.load(counter_file)\n",
    "with open('./data/f_t_counter.pkl', 'rb') as counter_file:\n",
    "    f_t_counter = pickle.load(counter_file)\n",
    "with open('./data/f_c_counter.pkl', 'rb') as counter_file:\n",
    "    f_c_counter = pickle.load(counter_file)\n",
    "N = sum([i for i in f_t_counter.values()])\n",
    "\n",
    "index_t = OrderedDict((key, i) for i, key in enumerate(f_t_counter.keys()))\n",
    "index_c = OrderedDict((key, i) for i, key in enumerate(f_c_counter.keys()))\n",
    "\n",
    "size_t = len(index_t)\n",
    "size_c = len(index_c)\n",
    "matrix = sparse.lil_matrix((size_t, size_c))\n",
    "\n",
    "for k, f_tc in f_tc_counter.items():\n",
    "    if f_tc >= 10:\n",
    "        t, c = k.split('\\t')\n",
    "        ppmi = max(np.log((N * f_tc) / (f_t_counter[t] * f_c_counter[c])), 0)\n",
    "        matrix[index_t[t], index_c[c]] = ppmi\n",
    "matrix = matrix.tocsc()\n",
    "sparse.save_npz('./data/matrix.npz', matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/index_t.pkl', 'wb') as index_file:\n",
    "    pickle.dump(index_t, index_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85. 主成分分析による次元削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "load_path = './data/matrix.npz'\n",
    "save_path = './data/matrix_low85'\n",
    "\n",
    "if not Path(save_path).exists():\n",
    "    word_matrix = sparse.load_npz(load_path)\n",
    "    pca = TruncatedSVD(n_components=300)\n",
    "    word_matrix300 = pca.fit_transform(word_matrix)\n",
    "    np.save(save_path, word_matrix300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383347, 300)\n"
     ]
    }
   ],
   "source": [
    "word_matrix300 = np.load(save_path+'.npy')\n",
    "print(word_matrix300.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 86. 単語ベクトルの表示\n",
    "\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"のベクトルを表示せよ．ただし，\"United States\"は内部的には\"United_States\"と表示されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "with open('./data/index_t.pkl', 'rb') as index_file:\n",
    "    index_t = pickle.load(index_file)\n",
    "index = index_t['United_States']\n",
    "word_vector = np.load('./data/matrix_low85.npy')\n",
    "United_States_vector = word_vector[index]\n",
    "print(United_States_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 87. 単語の類似度\n",
    "\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"と\"U.S.\"のコサイン類似度を計算せよ．ただし，\"U.S.\"は内部的に\"U.S\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8134705129640765"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cosine_simlarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "\n",
    "United_States_vector = word_vector[index_t['United_States']]\n",
    "US_vector = word_vector[index_t['U.S']]\n",
    "cosine_simlarity(United_States_vector, US_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 88. 類似度の高い単語10件\n",
    "85で得た単語の意味ベクトルを読み込み，\"England\"とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Anarchism', 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tazoe/anaconda3/envs/mxvis/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('France', 0.5604742705942972),\n",
       " ('central', 0.33396059620185314),\n",
       " ('represented', 0.2993267760110892),\n",
       " ('outside', 0.27016279255328934),\n",
       " ('United_States', 0.2505802752632198),\n",
       " ('left', 0.21779660131340314),\n",
       " ('first', 0.20664865603466295),\n",
       " ('world', 0.1996937918352727),\n",
       " ('William', 0.1938753158988156),\n",
       " ('himself', 0.15669713978002867)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "England_vector = word_vector[index_t['England']]\n",
    "print(list(index_t.items())[0])\n",
    "# cosine_simularities = [cosine_simularity(England_vector, word_vector[index]) or 0 for index in index_t.values()]\n",
    "cosine_similarities = []\n",
    "for t in index_t:\n",
    "    cosine_similarities.append((t, cosine_simlarity(England_vector, word_vector[index_t[t]])))\n",
    "sorted(cosine_similarities, key=lambda x: x[1], reverse=True)[:10]    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 89. 加法構成性によるアナロジー\n",
    "85で得た単語の意味ベクトルを読み込み，vec(\"Spain\") - vec(\"Madrid\") + vec(\"Athens\")を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tazoe/anaconda3/envs/mxvis/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('France', 0.7944462071837062),\n",
       " ('represented', 0.3354366695530093),\n",
       " ('self-defense', 0.29750623730744974),\n",
       " ('1593', 0.2687340426242362),\n",
       " ('participated', 0.2623910908085405),\n",
       " ('jail', 0.2506303550081883),\n",
       " ('propaganda', 0.2161203233700797),\n",
       " ('violent', 0.21222903454029549),\n",
       " ('differ', 0.20741492705377268),\n",
       " ('economics', 0.19909883629614705)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "wvector = word_vector[index_t['Spain']] + word_vector[index_t['Madrid']] + word_vector[index_t['Athens']]\n",
    "# cosine_simularities = [cosine_simularity(England_vector, word_vector[index]) or 0 for index in index_t.values()]\n",
    "cosine_similarities = []\n",
    "for t in index_t:\n",
    "    cosine_similarities.append((t, cosine_simlarity(wvector, word_vector[index_t[t]])))\n",
    "sorted(cosine_similarities, key=lambda x: x[1], reverse=True)[:10]    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ゴミ置き場"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def get_countriy_name():\n",
    "    url = 'https://www.worldometers.info/geography/alphabetical-list-of-countries/'\n",
    "    save_path = './data/countries.txt'\n",
    "    if os.path.exists(save_path):\n",
    "        return\n",
    "    # Setup Driver\n",
    "    print('Setup WebDriver')\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--blink-settings=imageEnabled=false')\n",
    "    browser = webdriver.Chrome(executable_path='./chromedriver', options=options)\n",
    "    browser.set_page_load_timeout(10*62)\n",
    "    \n",
    "    # Scraping\n",
    "    try:\n",
    "        browser.get(url)\n",
    "        table_contents = browser.find_element_by_xpath('//table[@class=\"table table-hover table-condensed\"]/tbody')\n",
    "        rows = table_contents.find_elements_by_tag_name('tr')\n",
    "        countries = '\\n'.join([row.find_elements_by_tag_name('td')[1].text for row in rows])\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write(countries)\n",
    "        print('Scraping is success.')\n",
    "        print(f'Save files at \"{save_path}\"')\n",
    "    except Exception as ERROR:\n",
    "        print(f'[TimeoutException]: {ERROR}')\n",
    "    browser.close()\n",
    "\n",
    "get_countriy_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
