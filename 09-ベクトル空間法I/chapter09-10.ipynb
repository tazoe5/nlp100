{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: ベクトル空間法 (I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80. コーパスの整形\n",
    "\n",
    "文を単語列に変換する最も単純な方法は，空白文字で単語に区切ることである．ただ，この方法では文末のピリオドや括弧などの記号が単語に含まれてしまう． そこで，コーパスの各行のテキストを空白文字でトークンのリストに分割した後，各トークンに以下の処理を施し，単語から記号を除去せよ．\n",
    "- トークンの先頭と末尾に出現する次の文字を削除: `.,!?;:()[]'\"`\n",
    "- 空文字列となったトークンは削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = []\n",
    "def get_corpus():\n",
    "    if Path('./data10/corpus80.txt').exists():\n",
    "        return\n",
    "    with open('./data10/enwiki-20150112-400-r10-105752.txt', 'r') as f_in, \\\n",
    "         open('./data10/corpus80.txt', 'w') as f_out:\n",
    "        corpus = []\n",
    "        for line in f_in.readlines():\n",
    "            line = [word.strip('.,!?:;()[]\\'\\\"') for word in line.split() \\\n",
    "                       if word.strip('.,!?:;()[]\\'\\\"')!='']\n",
    "            if len(line) >= 1:\n",
    "                corpus + line\n",
    "                f_out.write(' '.join(line)+'\\n')\n",
    "get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Anarchism\n",
      "1 Anarchism is a political philosophy that advocates stateless societies often defined as self-governed voluntary institutions but that several authors have defined as more specific institutions based on non-hierarchical free associations Anarchism holds the state to be undesirable unnecessary or harmful While anti-statism is central anarchism entails opposing authority or hierarchical organisation in the conduct of human relations including but not limited to the state system\n",
      "2 As a subtle and anti-dogmatic philosophy anarchism draws on many currents of thought and strategy Anarchism does not offer a fixed body of doctrine from a single particular world view instead fluxing and flowing as a philosophy There are many types and traditions of anarchism not all of which are mutually exclusive Anarchist schools of thought can differ fundamentally supporting anything from extreme individualism to complete collectivism Strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications Anarchism is usually considered a radical left-wing ideology and much of anarchist economics and anarchist legal philosophy reflect anti-authoritarian interpretations of communism collectivism syndicalism mutualism or participatory economics\n",
      "3 The central tendency of anarchism as a social movement has been represented by anarcho-communism and anarcho-syndicalism with individualist anarchism being primarily a literary phenomenon which nevertheless did have an impact on the bigger currents and individualists have also participated in large anarchist organisations Many anarchists oppose all forms of aggression supporting self-defense or non-violence anarcho-pacifism while others have supported the use of some coercive measures including violent revolution and propaganda of the deed as means to achieve anarchist ends\n",
      "4 Etymology and terminology\n"
     ]
    }
   ],
   "source": [
    "for line in zip(range(5), open('./data/corpus100_80.txt')):\n",
    "    print(*line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 81. 複合語からなる国名への対処\n",
    "英語では，複数の語の連接が意味を成すことがある．例えば，アメリカ合衆国は\"United States\"，イギリスは\"United Kingdom\"と表現されるが，\"United\"や\"States\"，\"Kingdom\"という単語だけでは，指し示している概念・実体が曖昧である．そこで，コーパス中に含まれる複合語を認識し，複合語を1語として扱うことで，複合語の意味を推定したい．しかしながら，複合語を正確に認定するのは大変むずかしいので，ここでは複合語からなる国名を認定したい\n",
    "\n",
    "~~国名データは[ここ](https://www.worldometers.info/geography/alphabetical-list-of-countries/)から手に入れる~~\n",
    "マン島がなくてダメだった．\n",
    "\n",
    "~~腹が立ったけど[ISO 3166-1に存在する国](http://www.asahi-net.or.jp/~ax2s-kmtn/ref/iso3166-1.html)から取得することにした．~~\n",
    "\n",
    "\n",
    "結局 `pip install pycounter`で対処"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def get_countriy_name():\n",
    "    url = 'https://www.listofcountriesoftheworld.com/'\n",
    "    save_path = './data/countries.txt'\n",
    "    if os.path.exists(save_path):\n",
    "        return\n",
    "    # Setup Driver\n",
    "    print('Setup WebDriver')\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--blink-settings=imageEnabled=false')\n",
    "    browser = webdriver.Chrome(executable_path='./chromedriver', options=options)\n",
    "    browser.set_page_load_timeout(10*62)\n",
    "    \n",
    "    # Scraping\n",
    "    try:\n",
    "        browser.get(url)\n",
    "        table_contents = browser.find_element_by_xpath('//*[@id=\"left-col\"]')\n",
    "        rows = table_contents.find_elements_by_xpath('//*[@id=\"ctry\"]') # //*[@id=\"ctry\"]\n",
    "        with open(save_path, 'w') as f:\n",
    "            for row in rows:\n",
    "                content = row.find_element_by_tag_name('a')\n",
    "                print(content)\n",
    "                f.write(content.text+'\\n')\n",
    "#        countries = '\\n'.join([row.find_elements_by_tag_name('td')[1].text for row in rows])\n",
    "        print('Scraping is success.')\n",
    "    except Exception as ERROR:\n",
    "        print(f'[TimeoutException]: {ERROR}')\n",
    "    browser.close()\n",
    "\n",
    "get_countriy_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "# from tqdm import tqdm_notebook, tqdm\n",
    "countries = list(pycountry.countries)\n",
    "countries_compound = [country.name for country in countries if ' ' in country.name]\n",
    "with open('./data10/countries.txt', 'w') as f:\n",
    "    f.write('\\n'.join(['_'.join(name.split()) for name in countries_compound]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data10/corpus81.txt', 'w')as f_81:\n",
    "    for line in open('./data10/corpus80.txt'):\n",
    "        for name in countries_compound:\n",
    "            if name in line:\n",
    "                line = line.replace(name, '_'.join(name.split()))\n",
    "        f_81.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The song is often included in songbooks in a wide variety of religious congregations in the United_States\n",
      "\n",
      "Politics of American_Samoa\n",
      "\n",
      "United_States\n",
      "\n",
      "New_Zealand\n",
      "\n",
      "Nevertheless his patronage led to the expansion of Buddhism in the Mauryan empire and other kingdoms during his rule and worldwide from about 250 BCE Prominent in this cause were his son Mahinda Mahendra and daughter Sanghamitra whose name means friend of the Sangha who established Buddhism in Ceylon now Sri_Lanka\n",
      "\n",
      "Other common names are ear shells sea ears and muttonfish or muttonshells in Australia ormer in Great Britain and in New_Zealand\n",
      "\n",
      "The haliotid family has a worldwide distribution along the coastal waters of every continent except the Pacific coast of South America the East Coast of the United_States the Arctic and Antarctica The majority of abalone species are found in cold waters such as off the coasts of New_Zealand South_Africa Australia Western North America and Japan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for line in open('./data10/corpus81.txt'):\n",
    "    for ctry in open('./data10/countries.txt'):\n",
    "        if ctry in line:\n",
    "            print(line)\n",
    "            n += 1\n",
    "            continue\n",
    "    if n == 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 82. 文脈の抽出\n",
    "81で作成したコーパス中に出現するすべての単語tに関して，単語tと文脈語cのペアをタブ区切り形式ですべて書き出せ．ただし，文脈語の定義は次の通りとする．\n",
    "\n",
    "ある単語tの前後d単語を文脈語cとして抽出する（ただし，文脈語に単語tそのものは含まない）\n",
    "単語tを選ぶ度に，文脈幅dは{1,2,3,4,5}の範囲でランダムに決める．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3bc18003c54143a03316e5ec114282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "\n",
    "with open('./data10/answer82.txt', 'w') as f_82:\n",
    "    for line in tqdm_notebook(open('./data10/corpus81.txt')):\n",
    "        words = line.split()\n",
    "        for index, t in enumerate(words):\n",
    "            d = random.randint(1, 5)\n",
    "            start = max(index - d, 0)\n",
    "            end = index + d + 1\n",
    "            for c in words[start:index] + words[index+1:end]:\n",
    "                f_82.write(f'{t}\\t{c}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\tis\n",
      "\n",
      "Anarchism\ta\n",
      "\n",
      "Anarchism\tpolitical\n",
      "\n",
      "is\tAnarchism\n",
      "\n",
      "is\ta\n",
      "\n",
      "is\tpolitical\n",
      "\n",
      "a\tAnarchism\n",
      "\n",
      "a\tis\n",
      "\n",
      "a\tpolitical\n",
      "\n",
      "a\tphilosophy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, context in enumerate(open('./data10/answer82.txt')):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 83. 単語／文脈の頻度の計測\n",
    "82の出力を利用し，以下の出現分布，および定数を求めよ．\n",
    "\n",
    "- f(t,c): 単語tと文脈語cの共起回数\n",
    "- f(t,∗): 単語tの出現回数\n",
    "- f(∗,c): 文脈語cの出現回数\n",
    "- N: 単語と文脈語のペアの総出現回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eceb00bef7a4d7aadf726b83338fe7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-67fd545c3c83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data10/f_tc_counter.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcounter_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_tc_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data10/f_t_counter.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcounter_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_t_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# f(t,c)\n",
    "f_tc_counter = Counter()\n",
    "f_t_counter = Counter()\n",
    "f_c_counter = Counter()\n",
    "\n",
    "tc_list = []\n",
    "t_list = []\n",
    "c_list = []\n",
    "num = 1000000\n",
    "for i ,line in enumerate(tqdm_notebook(open('./data10/answer82.txt'))):\n",
    "    tc = line.strip()\n",
    "    t, c = tc.split('\\t')\n",
    "    tc_list.append(tc)\n",
    "    t_list.append(t)\n",
    "    c_list.append(c)\n",
    "    \n",
    "    if i % num == 0:\n",
    "        f_tc_counter.update(tc_list)\n",
    "        tc_list = []\n",
    "        f_t_counter.update(t_list)\n",
    "        t_list = []\n",
    "        f_c_counter.update(c_list)\n",
    "        c_list = []\n",
    "f_tc_counter.update(tc_list)\n",
    "f_t_counter.update(t_list)\n",
    "f_c_counter.update(c_list)\n",
    "\n",
    "with open('./data10/f_tc_counter.pkl', 'wb') as counter_file:\n",
    "    pickle.dump(f_tc_counter, counter_file)\n",
    "with open('./data10/f_t_counter.pkl', 'wb') as counter_file:\n",
    "    pickle.dump(f_t_counter, counter_file)\n",
    "with open('./data10/f_c_counter.pkl', 'wb') as counter_file:\n",
    "    pickle.dump(f_c_counter, counter_file)\n",
    "print(f'N = {sum([i for i in f_t_counter.values()])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mac環境下で `pickle.dump`で4GB以上のデータを保存しようとするとエラーが出てしまったので， [Qiita](https://qiita.com/NomuraS/items/da3fd3a1ecd76175e5f8) を丸パクリすることで回避"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing total_bytes=2491441306...\n",
      "writing bytes [0, 1073741824)... done.\n",
      "writing bytes [1073741824, 2147483648)... done.\n",
      "writing bytes [2147483648, 2491441306)... done.\n",
      "writing total_bytes=25914858...\n",
      "writing bytes [0, 25914858)... done.\n",
      "writing total_bytes=25914780...\n",
      "writing bytes [0, 25914780)... done.\n",
      "N = 689461055\n"
     ]
    }
   ],
   "source": [
    "class MacOSFile(object):\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self.f, item)\n",
    "\n",
    "    def read(self, n):\n",
    "        # print(\"reading total_bytes=%s\" % n, flush=True)\n",
    "        if n >= (1 << 31):\n",
    "            buffer = bytearray(n)\n",
    "            idx = 0\n",
    "            while idx < n:\n",
    "                batch_size = min(n - idx, 1 << 31 - 1)\n",
    "                # print(\"reading bytes [%s,%s)...\" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "                buffer[idx:idx + batch_size] = self.f.read(batch_size)\n",
    "                # print(\"done.\", flush=True)\n",
    "                idx += batch_size\n",
    "            return buffer\n",
    "        return self.f.read(n)\n",
    "\n",
    "    def write(self, buffer):\n",
    "        n = len(buffer)\n",
    "        print(\"writing total_bytes=%s...\" % n, flush=True)\n",
    "        idx = 0\n",
    "        while idx < n:\n",
    "            batch_size = min(n - idx, 1 << 31 - 1)\n",
    "            print(\"writing bytes [%s, %s)... \" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "            self.f.write(buffer[idx:idx + batch_size])\n",
    "            print(\"done.\", flush=True)\n",
    "            idx += batch_size\n",
    "\n",
    "def pickle_dump(obj, file_path):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        return pickle.dump(obj, MacOSFile(f), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "def pickle_load(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(MacOSFile(f))\n",
    "\n",
    "\"\"\"\n",
    "with open('./data10/f_tc_counter.pkl', 'wb') as counter_file:\n",
    "    pickle_dump(f_tc_counter, counter_file)\n",
    "with open('./data10/f_t_counter.pkl', 'wb') as counter_file:\n",
    "    pickle_dump(f_t_counter, counter_file)\n",
    "with open('./data10/f_c_counter.pkl', 'wb') as counter_file:\n",
    "    pickle_dump(f_c_counter, counter_file)\n",
    "\"\"\"\n",
    "pickle_dump(f_tc_counter, './data10/f_tc_counter.pkl')\n",
    "pickle_dump(f_t_counter, './data10/f_t_counter.pkl')\n",
    "pickle_dump(f_c_counter, './data10/f_c_counter.pkl')\n",
    "print(f'N = {sum([i for i in f_t_counter.values()])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 84. 単語文脈行列の作成\n",
    "83の出力を利用し，単語文脈行列XXを作成せよ．ただし，行列XXの各要素XtcXtcは次のように定義する．\n",
    "\n",
    "- $f(t,c) \\geq 10$ ならば， $X_{tc} = PPMI(t,c) = \\max\\{\\log {\\frac{N \\times f(t,c)}{f(t,∗)\\times f(∗,c)}},0\\}$\n",
    "- $f(t,c)<10f(t,c)<10ならば，Xtc=0Xtc=0 $\n",
    "ここで，PPMI(tt,cc)はPositive Pointwise Mutual Information（正の相互情報量）と呼ばれる統計量である．なお，行列XXの行数・列数は数百万オーダとなり，行列のすべての要素を主記憶上に載せることは無理なので注意すること．幸い，行列XXのほとんどの要素は0になるので，非0の要素だけを書き出せばよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from scipy import sparse, io\n",
    "import pickle\n",
    "import numpy as np\n",
    "with open('./data10/f_tc_counter.pkl', 'rb') as counter_file:\n",
    "    f_tc_counter = pickle_load(counter_file)\n",
    "with open('./data10/f_t_counter.pkl', 'rb') as counter_file:\n",
    "    f_t_counter = pickle_load(counter_file)\n",
    "with open('./data10/f_c_counter.pkl', 'rb') as counter_file:\n",
    "    f_c_counter = pickle_load(counter_file)\n",
    "N = sum([i for i in f_t_counter.values()])\n",
    "\n",
    "index_t = OrderedDict((key, i) for i, key in enumerate(f_t_counter.keys()))\n",
    "index_c = OrderedDict((key, i) for i, key in enumerate(f_c_counter.keys()))\n",
    "\n",
    "size_t = len(index_t)\n",
    "size_c = len(index_c)\n",
    "matrix = sparse.lil_matrix((size_t, size_c))\n",
    "\n",
    "for k, f_tc in f_tc_counter.items():\n",
    "    if f_tc >= 10:\n",
    "        t, c = k.split('\\t')\n",
    "        ppmi = max(np.log((N * f_tc) / (f_t_counter[t] * f_c_counter[c])), 0)\n",
    "        matrix[index_t[t], index_c[c]] = ppmi\n",
    "matrix = matrix.tocsc()\n",
    "sparse.save_npz('./data10/matrix.npz', matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data10/index_t.pkl', 'wb') as index_file:\n",
    "    pickle.dump(index_t, index_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85. 主成分分析による次元削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "load_path = './data10/matrix.npz'\n",
    "save_path = './data10/matrix_low85'\n",
    "\n",
    "if not Path(save_path).exists():\n",
    "    word_matrix = sparse.load_npz(load_path)\n",
    "    pca = TruncatedSVD(n_components=300)\n",
    "    word_matrix300 = pca.fit_transform(word_matrix)\n",
    "    np.save(save_path, word_matrix300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1808580, 300)\n"
     ]
    }
   ],
   "source": [
    "# word_matrix300 = np.load(save_path+'.npy')\n",
    "print(word_matrix300.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 86. 単語ベクトルの表示\n",
    "\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"のベクトルを表示せよ．ただし，\"United States\"は内部的には\"United_States\"と表示されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data10/index_t.pkl', 'rb') as index_file:\n",
    "    index_t = pickle.load(index_file)\n",
    "word_vector = np.load('./data10/matrix_low85.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "index = index_t['United_States']\n",
    "United_States_vector = word_vector[index]\n",
    "print(United_States_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 87. 単語の類似度\n",
    "\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"と\"U.S.\"のコサイン類似度を計算せよ．ただし，\"U.S.\"は内部的に\"U.S\"と表現されていることに注意せよ．\n",
    "\n",
    "> 同じような文脈で使われていることがわかる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine_simlarity(vec1, vec2):\n",
    "    norm_ab = np.linalg.norm(vec1)*np.linalg.norm(vec2)\n",
    "    if norm_ab != 0:\n",
    "        return np.dot(vec1, vec2)/norm_ab\n",
    "    else: return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8494296352198178"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "United_States_vector = word_vector[index_t['United_States']]\n",
    "US_vector = word_vector[index_t['U.S']]\n",
    "cosine_simlarity(United_States_vector, US_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 88. 類似度の高い単語10件\n",
    "85で得た単語の意味ベクトルを読み込み，\"England\"とコサイン類似度が高い10語と，その類似度を出力せよ．\n",
    "\n",
    ">イングランドの地名が多い．オーストラリアやニュージーランドはなぜ？  \n",
    ">きちんとスコットランド，ウェールズ，アイルランドが入っているのがすごい．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Scotland', 0.7758510008207012),\n",
       " ('Wales', 0.7127774052389219),\n",
       " ('Australia', 0.6510417216056822),\n",
       " ('Ireland', 0.6438686528431701),\n",
       " (\"Yard's\", 0.6185058232994105),\n",
       " ('Yorkshire', 0.6118639381145149),\n",
       " ('Somerset', 0.6001176415423898),\n",
       " ('Britain', 0.5865217724281352),\n",
       " ('New_Zealand', 0.5751876740255513),\n",
       " ('Wiltshire', 0.5570767464552019)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "England_vector = word_vector[index_t['England']]\n",
    "# cosine_simularities = [cosine_simularity(England_vector, word_vector[index]) or 0 for index in index_t.values()]\n",
    "cosine_similarities = []\n",
    "for t in index_t:\n",
    "    cosine_similarities.append((t, cosine_simlarity(England_vector, word_vector[index_t[t]])))\n",
    "sorted(cosine_similarities, key=lambda x: x[1], reverse=True)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 89. 加法構成性によるアナロジー\n",
    "85で得た単語の意味ベクトルを読み込み，vec(\"Spain\") - vec(\"Madrid\") + vec(\"Athens\")を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．\n",
    "\n",
    ">**スペイン（国名） - マドリード（首都） $\\fallingdotseq$ ギリシャ - アテネ（ギリシャの首都）**  \n",
    ">なる関係が成り立つと考えられるので，結果に **Greece**が欲しかった．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spain', 0.9047012984375865),\n",
       " ('Portugal', 0.8242625537918804),\n",
       " ('Sweden', 0.7904240780085151),\n",
       " ('Italy', 0.7859304303471885),\n",
       " ('Netherlands', 0.7766299141193055),\n",
       " ('France', 0.7760652060704972),\n",
       " ('Norway', 0.7730416508507796),\n",
       " ('Belgium', 0.7718286086726056),\n",
       " ('Argentina', 0.7701562582024696),\n",
       " ('Denmark', 0.765747133804346)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "wvector = word_vector[index_t['Spain']] + word_vector[index_t['Madrid']] + word_vector[index_t['Athens']]\n",
    "cosine_similarities = []\n",
    "for t in index_t:\n",
    "    cosine_similarities.append((t, cosine_simlarity(wvector, word_vector[index_t[t]])))\n",
    "sorted(cosine_similarities, key=lambda x: x[1], reverse=True)[:10]    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ゴミ置き場"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def get_countriy_name():\n",
    "    url = 'https://www.worldometers.info/geography/alphabetical-list-of-countries/'\n",
    "    save_path = './data/countries.txt'\n",
    "    if os.path.exists(save_path):\n",
    "        return\n",
    "    # Setup Driver\n",
    "    print('Setup WebDriver')\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--blink-settings=imageEnabled=false')\n",
    "    browser = webdriver.Chrome(executable_path='./chromedriver', options=options)\n",
    "    browser.set_page_load_timeout(10*62)\n",
    "    \n",
    "    # Scraping\n",
    "    try:\n",
    "        browser.get(url)\n",
    "        table_contents = browser.find_element_by_xpath('//table[@class=\"table table-hover table-condensed\"]/tbody')\n",
    "        rows = table_contents.find_elements_by_tag_name('tr')\n",
    "        countries = '\\n'.join([row.find_elements_by_tag_name('td')[1].text for row in rows])\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write(countries)\n",
    "        print('Scraping is success.')\n",
    "        print(f'Save files at \"{save_path}\"')\n",
    "    except Exception as ERROR:\n",
    "        print(f'[TimeoutException]: {ERROR}')\n",
    "    browser.close()\n",
    "\n",
    "get_countriy_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
