{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import re\n",
    "import CaboCha\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/neko.txt') as input_neko, \\\n",
    "         open('data/neko.txt.cabocha', mode='w') as output_neko:\n",
    "    cabocha = CaboCha.Parser()\n",
    "    for line in input_neko:\n",
    "        output_neko.write(\n",
    "            cabocha.parse(line).toString(CaboCha.FORMAT_LATTICE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40. 係り受け解析結果の読み込み（形態素）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "    def __str__(self):\n",
    "        return 'Morph({}, [{}, {}, {}])'.format(self.surface, self.base, self.pos, self.pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Morph(\\u3000, [\\u3000, 記号, 空白])', 'Morph(どこ, [どこ, 名詞, 代名詞])', 'Morph(で, [で, 助詞, 格助詞])', 'Morph(生れ, [生れる, 動詞, 自立])', 'Morph(た, [た, 助動詞, *])', 'Morph(か, [か, 助詞, 副助詞／並立助詞／終助詞])', 'Morph(とんと, [とんと, 副詞, 一般])', 'Morph(見当, [見当, 名詞, サ変接続])', 'Morph(が, [が, 助詞, 格助詞])', 'Morph(つか, [つく, 動詞, 自立])', 'Morph(ぬ, [ぬ, 助動詞, *])', 'Morph(。, [。, 記号, 句点])']\n"
     ]
    }
   ],
   "source": [
    "sentence = []\n",
    "sentences = []\n",
    "with open('data/neko.txt.cabocha') as neko_cabocha:\n",
    "    for line in neko_cabocha:\n",
    "        m = re.search(r'(?P<surface>.*?)\\t(?P<pos>[^,]*),(?P<pos1>[^,]*),[^,]*,[^,]*,[^,]*,[^,]*,(?P<base>[^,]*).*', line)\n",
    "        if m:\n",
    "            sentence.append(Morph(surface=m.group('surface'), base=m.group('base'), \n",
    "                                    pos=m.group('pos'), pos1=m.group('pos1')))\n",
    "        elif line=='EOS\\n' and not sentence == []:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "print([str(morph) for morph in sentences[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, morphs, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "    def __str__(self):\n",
    "        srcs = self.srcs\n",
    "        dst = self.dst\n",
    "        morphs = ''.join([morph.surface for morph in self.morphs])\n",
    "        return f'dst: {dst}, Morphs: {morphs}, srcs: {srcs}'\n",
    "    def to_string(self):\n",
    "        return ''.join([morph.surface for morph in self.morphs])\n",
    "    def to_normalized_string(self):\n",
    "        return ''.join([morph.surface for morph in self.morphs if morph.pos!='記号'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9210\n",
      "['この -> 書生というのは', '書生というのは -> 話である。', '時々 -> 捕えて', '我々を -> 捕えて', '捕えて -> 煮て', '煮て -> 食うという', '食うという -> 話である。', '話である。 -> 話である。']\n"
     ]
    }
   ],
   "source": [
    "with open('data/neko.txt.cabocha') as neko_cabocha:\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    chunk = None\n",
    "    morphs, dst, srcs = [], None, []\n",
    "    for line in neko_cabocha:\n",
    "        # 係り受け情報を取得し，Chunkオブジェクトを作成する\n",
    "        m1 = re.search(r'\\*\\ (?P<phrase_id>[0-9]+)\\ (?P<dst>-?[0-9]*)D.*', line)\n",
    "        if m1:\n",
    "            dst = m1.group('dst')\n",
    "            phrase_id = m1.group('phrase_id')\n",
    "            srcs = [p_id for (p_id, chunk) in enumerate(sentence) if chunk.dst == phrase_id]\n",
    "            chunk = Chunk(morphs=[], dst=dst, srcs=srcs)\n",
    "            sentence.append(chunk)        \n",
    "        # 形態素を取得\n",
    "        m = re.search(r'(?P<surface>.*?)\\t(?P<pos>[^,]*),(?P<pos1>[^,]*),[^,]*,[^,]*,[^,]*,[^,]*,(?P<base>[^,]*),.*', line)\n",
    "        if m:\n",
    "            sentence[-1].morphs.append(Morph(surface=m.group('surface'), base=m.group('base'), \n",
    "                                    pos=m.group('pos'), pos1=m.group('pos1')))\n",
    "            continue\n",
    "        elif line == 'EOS\\n' and sentence != []:\n",
    "            sentences.append(sentence)\n",
    "            sentence, dst, srcs = [], None, []\n",
    "print(len(sentences))\n",
    "sentence = sentences[7]\n",
    "print([f'{chunk.to_string()} -> {sentence[int(chunk.dst)].to_string()}' \\\n",
    "       if chunk.dst!=-1 else str(chunk) for chunk in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42. 係り元と係り先の文節の表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aaaget_dependency_phrase(sentence):\n",
    "    for chunk in sentence:\n",
    "        if chunk.dst == '-1': \n",
    "            yield f'「{chunk.to_normalized_string()}」'\n",
    "            continue\n",
    "        yield f'「{chunk.to_normalized_string()}」 \\t 「{sentence[int(chunk.dst)].to_normalized_string()}」'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isin_part_of_speech(chunk, name) -> bool:\n",
    "    if name:\n",
    "        return True\n",
    "    for morph in chunk.morphs:\n",
    "        if morph.pos == name:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_dependency_phrase(sentence, n=None, first_part=True, second_part=True):\n",
    "    n = n or len(sentence)\n",
    "    for i, chunk in enumerate(sentence):\n",
    "        if i == n:\n",
    "            break\n",
    "        if chunk.dst == '-1':\n",
    "#            yield f'「{chunk.to_normalized_string()}」'\n",
    "            continue   \n",
    "        if isin_part_of_speech(chunk, first_part) and isin_part_of_speech(sentence[int(chunk.dst)], second_part):\n",
    "            n += 1\n",
    "            yield f'「{chunk.to_normalized_string()}」 \\t 「{sentence[int(chunk.dst)].to_normalized_string()}」'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['何でも', '薄暗い', 'じめじめした', '所で', '', '泣いて', 'いた事だけは', '記憶している']\n",
      "「何でも」 \t 「薄暗い」\n",
      "「薄暗い」 \t 「所で」\n",
      "「じめじめした」 \t 「所で」\n",
      "「所で」 \t 「泣いて」\n",
      "「」 \t 「泣いて」\n",
      "「泣いて」 \t 「記憶している」\n",
      "「いた事だけは」 \t 「記憶している」\n",
      "「記憶している」\n",
      "\n",
      "「何でも」 \t 「薄暗い」\n",
      "「薄暗い」 \t 「所で」\n",
      "「じめじめした」 \t 「所で」\n",
      "「所で」 \t 「泣いて」\n",
      "「」 \t 「泣いて」\n",
      "「泣いて」 \t 「記憶している」\n",
      "「いた事だけは」 \t 「記憶している」\n"
     ]
    }
   ],
   "source": [
    "r = 4\n",
    "print(f'text: {[chunk.to_normalized_string() for chunk in sentences[r]]}')\n",
    "\n",
    "for i,text in enumerate(aaaget_dependency_phrase(sentences[r])):\n",
    "    if i > 10: break\n",
    "    print(text)\n",
    "print()\n",
    "print('\\n'.join([ i for i in get_dependency_phrase(sentences[r], 10, True, True)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43. 名詞を含む文節が動詞を含む文節に係るものを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「ただ」 \t 「載せられて」\n",
      "「彼の」 \t 「掌に」\n",
      "「掌に」 \t 「載せられて」\n",
      "「載せられて」 \t 「持ち上げられた」\n",
      "「スーと」 \t 「持ち上げられた」\n",
      "「持ち上げられた」 \t 「時」\n",
      "「時」 \t 「フワフワした」\n",
      "「何だか」 \t 「フワフワした」\n",
      "「フワフワした」 \t 「感じが」\n",
      "「感じが」 \t 「あったばかりである」\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([ i for i in get_dependency_phrase(sentences[9], 10, '名詞', '動詞')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 44. 係り受け木の可視化\n",
    "例)\n",
    "```\n",
    "digraph graphname {\n",
    "     a -> b -> c;\n",
    "     b -> d;\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
